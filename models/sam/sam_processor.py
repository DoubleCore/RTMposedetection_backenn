"""
SAMå¤„ç†å™¨
ä¸“é—¨ç”¨äºåˆ†ç¦»äººç‰©å’Œæ¸¸æ³³æ± èƒŒæ™¯çš„FastSAMåˆ†å‰²å¤„ç†
"""
import cv2
import numpy as np
import os
import json
from typing import List, Dict, Any, Optional, Tuple
import logging
from datetime import datetime

from .fast_sam import get_fast_sam_segmenter

logger = logging.getLogger(__name__)

class SAMProcessor:
    """
    SAMå¤„ç†å™¨ - ä¸“é—¨åˆ†ç¦»äººç‰©å’Œæ¸¸æ³³æ± èƒŒæ™¯
    """
    
    def __init__(self, output_dir: str = "output"):
        """
        åˆå§‹åŒ–SAMå¤„ç†å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = output_dir
        self.pose_dir = os.path.join(output_dir, "pose")
        self.sam_dir = os.path.join(output_dir, "sam")
        self.json_dir = os.path.join(output_dir, "json")
        
        # ç¡®ä¿SAMè¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.sam_dir, exist_ok=True)
        
        # è·å–FastSAMåˆ†å‰²å™¨
        self.segmenter = get_fast_sam_segmenter()
        
        logger.info(f"SAMå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œä¸“é—¨ç”¨äºäººç‰©å’Œæ¸¸æ³³æ± èƒŒæ™¯åˆ†ç¦»")
    
    def process_all_frames(self, processing_mode: str = "separate_person_pool") -> Dict[str, Any]:
        """
        å¤„ç†æ‰€æœ‰å¸§çš„äººç‰©å’Œæ¸¸æ³³æ± èƒŒæ™¯åˆ†ç¦»
        
        Args:
            processing_mode: å¤„ç†æ¨¡å¼
                - "separate_person_pool": åˆ†ç¦»äººç‰©å’Œæ¸¸æ³³æ± 
                - "highlight_person": çªå‡ºäººç‰©ï¼Œæ¨¡ç³ŠèƒŒæ™¯
                - "extract_person": æå–äººç‰©ï¼Œé€æ˜èƒŒæ™¯
                - "pool_only": åªä¿ç•™æ¸¸æ³³æ± ï¼Œç§»é™¤äººç‰©
                
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        try:
            logger.info(f"ğŸ”„ å¼€å§‹äººç‰©å’Œæ¸¸æ³³æ± èƒŒæ™¯åˆ†ç¦»å¤„ç†ï¼Œæ¨¡å¼: {processing_mode}")
            
            # æŸ¥æ‰¾æ‰€æœ‰poseå›¾ç‰‡å’Œå¯¹åº”çš„JSONæ–‡ä»¶
            pose_files = []
            for filename in os.listdir(self.pose_dir):
                if filename.startswith("frame_") and filename.endswith(".jpg"):
                    pose_files.append(filename)
            
            pose_files.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))
            
            processed_count = 0
            error_count = 0
            person_detected_count = 0
            
            for pose_filename in pose_files:
                try:
                    # æå–å¸§å·
                    frame_number = int(pose_filename.split('_')[1].split('.')[0])
                    
                    # å¤„ç†å•ä¸ªå¸§
                    result = self.process_single_frame(frame_number, processing_mode)
                    
                    if result["success"]:
                        processed_count += 1
                        if result.get("person_detected", False):
                            person_detected_count += 1
                        
                        # è¿›åº¦æ—¥å¿—
                        if frame_number % 50 == 0:
                            logger.info(f"ğŸ¯ å·²å¤„ç† {processed_count} å¸§ï¼Œæ£€æµ‹åˆ°äººç‰© {person_detected_count} å¸§...")
                        elif frame_number <= 10:
                            logger.info(f"ğŸ¯ å¤„ç†å¸§ {frame_number}: äººç‰©={'æ˜¯' if result.get('person_detected') else 'å¦'}")
                    else:
                        error_count += 1
                        
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†å¸§ {pose_filename} å¤±è´¥: {str(e)}")
                    error_count += 1
                    continue
            
            logger.info(f"âœ… äººç‰©å’Œæ¸¸æ³³æ± åˆ†ç¦»å®Œæˆ: æˆåŠŸ {processed_count} å¸§, äººç‰©æ£€æµ‹ {person_detected_count} å¸§, å¤±è´¥ {error_count} å¸§")
            
            return {
                "total_processed": processed_count,
                "person_detected_frames": person_detected_count,
                "total_errors": error_count,
                "processing_mode": processing_mode,
                "output_directory": self.sam_dir
            }
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")
            return {"error": str(e)}
    
    def process_single_frame(self, frame_number: int, processing_mode: str = "separate_person_pool") -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªå¸§çš„äººç‰©å’Œæ¸¸æ³³æ± èƒŒæ™¯åˆ†ç¦»
        
        Args:
            frame_number: å¸§å·
            processing_mode: å¤„ç†æ¨¡å¼
            
        Returns:
            å¤„ç†ç»“æœ
        """
        try:
            # æ„å»ºæ–‡ä»¶è·¯å¾„
            pose_filename = f"frame_{frame_number}.jpg"
            json_filename = f"frame_{frame_number}.json"
            
            pose_path = os.path.join(self.pose_dir, pose_filename)
            json_path = os.path.join(self.json_dir, json_filename)
            
            # è¯»å–åŸå§‹å›¾ç‰‡ï¼ˆä¼˜å…ˆä»originç›®å½•ï¼‰
            origin_path = os.path.join(self.output_dir, "origin", pose_filename)
            if os.path.exists(origin_path):
                image = cv2.imread(origin_path)
                logger.info(f"ğŸ¯ å¤„ç†å¸§ {frame_number}: ä½¿ç”¨åŸå§‹å›¾ç‰‡ {origin_path}")
            else:
                image = cv2.imread(pose_path)
                logger.info(f"ğŸ¯ å¤„ç†å¸§ {frame_number}: ä½¿ç”¨poseå›¾ç‰‡ {pose_path}")
            
            if image is None:
                logger.error(f"âŒ æ— æ³•è¯»å–å¸§ {frame_number} çš„å›¾åƒ")
                return {"success": False, "error": f"æ— æ³•è¯»å–å›¾åƒ"}
            
            # è¯»å–poseæ£€æµ‹ç»“æœ
            pose_data = None
            if os.path.exists(json_path):
                with open(json_path, 'r', encoding='utf-8') as f:
                    pose_data = json.load(f)
                logger.info(f"ğŸ“‹ å¸§ {frame_number}: è¯»å–åˆ°poseæ•°æ®ï¼ŒåŒ…å« {len(pose_data.get('persons', []))} ä¸ªäººç‰©")
            else:
                logger.warning(f"âš ï¸ å¸§ {frame_number}: æœªæ‰¾åˆ°poseæ•°æ®æ–‡ä»¶ {json_path}")
            
            # ä½¿ç”¨FastSAMè¿›è¡Œå…¨å›¾åˆ†å‰²
            logger.info(f"ğŸ”„ å¸§ {frame_number}: å¼€å§‹FastSAMåˆ†å‰²...")
            segment_result = self.segmenter.segment_everything(image)
            logger.info(f"âœ… å¸§ {frame_number}: FastSAMåˆ†å‰²å®Œæˆï¼Œæ£€æµ‹åˆ° {segment_result['num_segments']} ä¸ªåŒºåŸŸ")
            
            # åˆ†æåˆ†å‰²ç»“æœï¼Œè¯†åˆ«äººç‰©å’Œæ¸¸æ³³æ± åŒºåŸŸ
            logger.info(f"ğŸ” å¸§ {frame_number}: å¼€å§‹è¯†åˆ«äººç‰©å’Œæ¸¸æ³³æ± åŒºåŸŸ...")
            person_mask, pool_mask = self._identify_person_and_pool_regions(
                image, segment_result, pose_data
            )
            
            person_detected = np.any(person_mask > 0) if person_mask is not None else False
            pool_detected = np.any(pool_mask > 0) if pool_mask is not None else False
            
            logger.info(f"ğŸ¯ å¸§ {frame_number}: è¯†åˆ«ç»“æœ - äººç‰©: {'âœ…' if person_detected else 'âŒ'}, æ¸¸æ³³æ± : {'âœ…' if pool_detected else 'âŒ'}")
            
            # æ ¹æ®å¤„ç†æ¨¡å¼ç”Ÿæˆç»“æœå›¾åƒ
            logger.info(f"ğŸ–¼ï¸ å¸§ {frame_number}: ç”Ÿæˆ {processing_mode} æ¨¡å¼çš„å¤„ç†å›¾åƒ...")
            result_images = self._generate_processed_images(
                image, person_mask, pool_mask, processing_mode
            )
            
            # ä¿å­˜å¤„ç†ç»“æœ
            saved_files = self._save_processing_results(
                frame_number, result_images, person_mask, pool_mask, 
                segment_result, processing_mode
            )
            
            logger.info(f"ğŸ’¾ å¸§ {frame_number}: ä¿å­˜äº† {len(saved_files)} ä¸ªæ–‡ä»¶")
            
            return {
                "success": True,
                "frame_number": frame_number,
                "person_detected": person_detected,
                "pool_detected": pool_detected,
                "processing_mode": processing_mode,
                "saved_files": saved_files,
                "num_segments": segment_result["num_segments"],
                "debug_info": {
                    "image_source": "origin" if os.path.exists(origin_path) else "pose",
                    "pose_data_available": pose_data is not None,
                    "segments_detected": segment_result["num_segments"],
                    "result_images_generated": len(result_images)
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¸§ {frame_number} å¤±è´¥: {str(e)}")
            return {"success": False, "error": str(e), "frame_number": frame_number}
    
    def _identify_person_and_pool_regions(self, image: np.ndarray, segment_result: Dict, 
                                        pose_data: Optional[Dict]) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
        """
        è¯†åˆ«äººç‰©å’Œæ¸¸æ³³æ± åŒºåŸŸ
        
        Args:
            image: åŸå§‹å›¾åƒ
            segment_result: FastSAMåˆ†å‰²ç»“æœ
            pose_data: poseæ£€æµ‹æ•°æ®
            
        Returns:
            (person_mask, pool_mask) å…ƒç»„
        """
        try:
            height, width = image.shape[:2]
            person_mask = np.zeros((height, width), dtype=np.uint8)
            pool_mask = np.zeros((height, width), dtype=np.uint8)
            
            if not segment_result["masks"]:
                return None, None
            
            # å¦‚æœæœ‰poseæ•°æ®ï¼Œä¼˜å…ˆä½¿ç”¨poseä¿¡æ¯è¯†åˆ«äººç‰©
            person_regions = []
            if pose_data and "persons" in pose_data:
                person_regions = self._extract_person_regions_from_pose(pose_data, image.shape[:2])
            
            # åˆ†ææ¯ä¸ªåˆ†å‰²åŒºåŸŸ
            for i, mask in enumerate(segment_result["masks"]):
                if i >= len(segment_result["segments"]):
                    continue
                    
                segment_info = segment_result["segments"][i]
                mask_resized = cv2.resize(mask, (width, height))
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºäººç‰©åŒºåŸŸ
                if self._is_person_region(mask_resized, segment_info, person_regions, image):
                    person_mask = np.logical_or(person_mask, mask_resized > 0)
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºæ¸¸æ³³æ± åŒºåŸŸï¼ˆå¤§é¢ç§¯ï¼Œè“è‰²è°ƒï¼Œåœ¨åº•éƒ¨åŒºåŸŸï¼‰
                elif self._is_pool_region(mask_resized, segment_info, image):
                    pool_mask = np.logical_or(pool_mask, mask_resized > 0)
            
            # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°æ˜æ˜¾çš„æ¸¸æ³³æ± ï¼Œå°†éäººç‰©åŒºåŸŸä½œä¸ºèƒŒæ™¯
            if not np.any(pool_mask):
                pool_mask = ~person_mask
            
            return person_mask.astype(np.uint8), pool_mask.astype(np.uint8)
            
        except Exception as e:
            logger.error(f"è¯†åˆ«äººç‰©å’Œæ¸¸æ³³æ± åŒºåŸŸå¤±è´¥: {str(e)}")
            return None, None
    
    def _extract_person_regions_from_pose(self, pose_data: Dict, image_size: Tuple[int, int]) -> List[np.ndarray]:
        """
        ä»poseæ•°æ®ä¸­æå–äººç‰©åŒºåŸŸ
        """
        person_regions = []
        height, width = image_size
        
        try:
            for person in pose_data.get("persons", []):
                if "keypoints" in person and "scores" in person:
                    keypoints = person["keypoints"]
                    scores = person["scores"]
                    
                    # æ‰¾åˆ°æœ‰æ•ˆå…³é”®ç‚¹
                    valid_points = []
                    for i, (x, y) in enumerate(keypoints):
                        if i < len(scores) and scores[i] > 0.1:
                            valid_points.append((int(x), int(y)))
                    
                    if len(valid_points) >= 3:
                        # åˆ›å»ºäººç‰©åŒºåŸŸmask
                        person_mask = np.zeros((height, width), dtype=np.uint8)
                        
                        # è®¡ç®—è¾¹ç•Œæ¡†å¹¶æ‰©å±•
                        x_coords = [pt[0] for pt in valid_points]
                        y_coords = [pt[1] for pt in valid_points]
                        
                        margin = 50  # å¢å¤§è¾¹è·ä»¥åŒ…å«æ›´å¤šäººç‰©åŒºåŸŸ
                        x1 = max(0, min(x_coords) - margin)
                        y1 = max(0, min(y_coords) - margin)
                        x2 = min(width, max(x_coords) + margin)
                        y2 = min(height, max(y_coords) + margin)
                        
                        person_mask[y1:y2, x1:x2] = 255
                        person_regions.append(person_mask)
            
        except Exception as e:
            logger.error(f"æå–äººç‰©åŒºåŸŸå¤±è´¥: {str(e)}")
        
        return person_regions
    
    def _is_person_region(self, mask: np.ndarray, segment_info: Dict, 
                         person_regions: List[np.ndarray], image: np.ndarray) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºäººç‰©åŒºåŸŸ
        """
        try:
            # å¦‚æœæœ‰poseä¿¡æ¯ï¼Œæ£€æŸ¥ä¸poseåŒºåŸŸçš„é‡å 
            if person_regions:
                for person_region in person_regions:
                    overlap = np.logical_and(mask > 0, person_region > 0)
                    overlap_ratio = np.sum(overlap) / max(int(np.sum(mask > 0)), 1)
                    if overlap_ratio > 0.2:  # é™ä½åˆ°20%ä»¥ä¸Šé‡å è®¤ä¸ºæ˜¯äººç‰©
                        return True
            
            # åŸºäºåŒºåŸŸç‰¹å¾åˆ¤æ–­
            area = segment_info.get("area", 0)
            bbox = segment_info.get("bbox", [0, 0, 0, 0])
            
            # äººç‰©é€šå¸¸ä¸ä¼šå æ®æ•´ä¸ªå›¾åƒï¼Œä¸”æœ‰åˆç†çš„å®½é«˜æ¯”
            image_area = image.shape[0] * image.shape[1]
            area_ratio = area / image_area
            
            # æ”¾å®½é¢ç§¯æ¡ä»¶ï¼š0.005-50%ä¹‹é—´éƒ½å¯èƒ½æ˜¯äººç‰©
            if 0.005 < area_ratio < 0.5:  
                width = bbox[2] - bbox[0]
                height = bbox[3] - bbox[1]
                if height > 0:
                    aspect_ratio = width / height
                    # æ”¾å®½å®½é«˜æ¯”ï¼š0.2-3.0ä¹‹é—´
                    if 0.2 < aspect_ratio < 3.0:
                        # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœåŒºåŸŸä¸åœ¨å›¾åƒè¾¹ç¼˜ï¼Œæ›´å¯èƒ½æ˜¯äººç‰©
                        center_x = (bbox[0] + bbox[2]) / 2
                        center_y = (bbox[1] + bbox[3]) / 2
                        
                        # å¦‚æœåŒºåŸŸä¸­å¿ƒä¸åœ¨å›¾åƒçš„æè¾¹ç¼˜ä½ç½®ï¼Œè®¤ä¸ºæ˜¯äººç‰©
                        if (0.05 * image.shape[1] < center_x < 0.95 * image.shape[1] and 
                            0.05 * image.shape[0] < center_y < 0.95 * image.shape[0]):
                            return True
                        
                        # å¦‚æœé¢ç§¯è¶³å¤Ÿå¤§ï¼Œä¹Ÿå¯èƒ½æ˜¯äººç‰©
                        if area_ratio > 0.02:
                            return True
            
            return False
            
        except Exception as e:
            logger.error(f"åˆ¤æ–­äººç‰©åŒºåŸŸå¤±è´¥: {str(e)}")
            return False
    
    def _is_pool_region(self, mask: np.ndarray, segment_info: Dict, image: np.ndarray) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºæ¸¸æ³³æ± åŒºåŸŸ
        """
        try:
            area = segment_info.get("area", 0)
            bbox = segment_info.get("bbox", [0, 0, 0, 0])
            
            # æ¸¸æ³³æ± é€šå¸¸å æ®è¾ƒå¤§é¢ç§¯
            image_area = image.shape[0] * image.shape[1]
            area_ratio = area / image_area
            
            # é™ä½é¢ç§¯è¦æ±‚ï¼š15%ä»¥ä¸Š
            if area_ratio > 0.15:  
                # æ£€æŸ¥åŒºåŸŸæ˜¯å¦ä¸»è¦åœ¨å›¾åƒä¸‹åŠéƒ¨åˆ†ï¼ˆæ¸¸æ³³æ± é€šå¸¸åœ¨ä¸‹æ–¹ï¼‰
                y_center = (bbox[1] + bbox[3]) / 2
                if y_center > image.shape[0] * 0.3:  # é‡å¿ƒåœ¨å›¾åƒä¸‹éƒ¨30%ä»¥ä¸‹
                    
                    # åˆ†æé¢œè‰²ç‰¹å¾ï¼ˆæ¸¸æ³³æ± é€šå¸¸æ˜¯è“è‰²è°ƒï¼‰
                    masked_region = image[mask > 0]
                    if len(masked_region) > 0:
                        # è®¡ç®—è“è‰²é€šé“çš„å‡å€¼
                        mean_bgr = np.mean(masked_region, axis=0)
                        blue_ratio = mean_bgr[0] / (np.sum(mean_bgr) + 1e-6)  # Bé€šé“å æ¯”
                        
                        # é™ä½è“è‰²å æ¯”è¦æ±‚ï¼š30%ä»¥ä¸Šæˆ–è€…æ˜¯å¤§é¢ç§¯çš„èƒŒæ™¯åŒºåŸŸ
                        if blue_ratio > 0.3 or area_ratio > 0.4:
                            return True
                        
                        # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœæ˜¯éå¸¸å¤§çš„åŒºåŸŸï¼ˆå¯èƒ½æ˜¯èƒŒæ™¯ï¼‰
                        if area_ratio > 0.6:
                            return True
            
            # å¦‚æœæ˜¯å æ®å›¾åƒå¤§éƒ¨åˆ†ä¸”ä½ç½®è¾ƒä½çš„åŒºåŸŸï¼Œä¹Ÿè®¤ä¸ºæ˜¯æ¸¸æ³³æ± 
            if area_ratio > 0.3:
                bbox_bottom = bbox[3]
                if bbox_bottom > image.shape[0] * 0.7:  # åº•éƒ¨è¾¹ç•Œåœ¨å›¾åƒä¸‹æ–¹70%ä»¥ä¸‹
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"åˆ¤æ–­æ¸¸æ³³æ± åŒºåŸŸå¤±è´¥: {str(e)}")
            return False
    
    def _generate_processed_images(self, image: np.ndarray, person_mask: Optional[np.ndarray], 
                                 pool_mask: Optional[np.ndarray], processing_mode: str) -> Dict[str, np.ndarray]:
        """
        æ ¹æ®å¤„ç†æ¨¡å¼ç”Ÿæˆç»“æœå›¾åƒ
        """
        result_images = {}
        
        try:
            if person_mask is None or pool_mask is None:
                result_images["original"] = image.copy()
                return result_images
            
            if processing_mode == "separate_person_pool":
                # æ¨¡å¼1ï¼šåˆ†ç¦»äººç‰©å’Œæ¸¸æ³³æ± 
                # äººç‰©å›¾åƒï¼ˆç™½è‰²èƒŒæ™¯ï¼‰
                person_img = image.copy()
                person_img[person_mask == 0] = [255, 255, 255]
                result_images["person_only"] = person_img
                
                # æ¸¸æ³³æ± å›¾åƒï¼ˆç§»é™¤äººç‰©ï¼‰
                pool_img = image.copy()
                pool_img[person_mask > 0] = [255, 255, 255]
                result_images["pool_only"] = pool_img
                
                # ç»„åˆå›¾åƒï¼ˆå·¦ï¼šäººç‰©ï¼Œå³ï¼šæ¸¸æ³³æ± ï¼‰
                h, w = image.shape[:2]
                combined = np.zeros((h, w*2, 3), dtype=np.uint8)
                combined[:, :w] = person_img
                combined[:, w:] = pool_img
                result_images["combined"] = combined
            
            elif processing_mode == "highlight_person":
                # æ¨¡å¼2ï¼šçªå‡ºäººç‰©ï¼Œæ¨¡ç³Šæ¸¸æ³³æ± èƒŒæ™¯
                highlighted = image.copy()
                
                # æ¨¡ç³ŠèƒŒæ™¯
                blurred_bg = cv2.GaussianBlur(image, (21, 21), 0)
                highlighted[person_mask == 0] = blurred_bg[person_mask == 0]
                
                result_images["highlighted_person"] = highlighted
            
            elif processing_mode == "extract_person":
                # æ¨¡å¼3ï¼šæå–äººç‰©ï¼Œé€æ˜èƒŒæ™¯
                person_extracted = image.copy()
                person_extracted = cv2.cvtColor(person_extracted, cv2.COLOR_BGR2BGRA)
                person_extracted[person_mask == 0, 3] = 0  # è®¾ç½®é€æ˜
                
                result_images["person_extracted"] = person_extracted
            
            elif processing_mode == "pool_only":
                # æ¨¡å¼4ï¼šåªä¿ç•™æ¸¸æ³³æ± ï¼Œç§»é™¤äººç‰©
                pool_only = image.copy()
                
                # ä½¿ç”¨èƒŒæ™¯ä¿®å¤æŠ€æœ¯å¡«å……äººç‰©åŒºåŸŸ
                if np.any(person_mask > 0):
                    inpaint_mask = person_mask.copy()
                    pool_only = cv2.inpaint(pool_only, inpaint_mask, 3, cv2.INPAINT_TELEA)
                
                result_images["pool_only"] = pool_only
            
            # æ€»æ˜¯ç”ŸæˆåŸå›¾å’Œmaskå¯è§†åŒ–
            result_images["original"] = image.copy()
            
            # åˆ›å»ºmaskå¯è§†åŒ–
            mask_viz = np.zeros_like(image)
            mask_viz[person_mask > 0] = [0, 255, 0]  # ç»¿è‰²è¡¨ç¤ºäººç‰©
            mask_viz[pool_mask > 0] = [255, 0, 0]   # è“è‰²è¡¨ç¤ºæ¸¸æ³³æ± 
            result_images["mask_visualization"] = mask_viz
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¤„ç†å›¾åƒå¤±è´¥: {str(e)}")
            result_images["original"] = image.copy()
        
        return result_images
    
    def _save_processing_results(self, frame_number: int, result_images: Dict[str, np.ndarray],
                               person_mask: Optional[np.ndarray], pool_mask: Optional[np.ndarray],
                               segment_result: Dict, processing_mode: str) -> List[str]:
        """
        ä¿å­˜å¤„ç†ç»“æœ
        """
        saved_files = []
        
        try:
            # ä¿å­˜å„ç§ç»“æœå›¾åƒ
            for image_type, image_data in result_images.items():
                filename = f"frame_{frame_number}_{image_type}.jpg"
                filepath = os.path.join(self.sam_dir, filename)
                
                # å¤„ç†RGBAå›¾åƒ
                if image_data.shape[-1] == 4:  # RGBA
                    cv2.imwrite(filepath.replace('.jpg', '.png'), image_data)
                    saved_files.append(filename.replace('.jpg', '.png'))
                else:
                    cv2.imwrite(filepath, image_data)
                    saved_files.append(filename)
            
            # ä¿å­˜JSONä¿¡æ¯
            json_info = {
                "frame_number": frame_number,
                "processing_time": datetime.now().isoformat(),
                "processing_mode": processing_mode,
                "person_detected": np.any(person_mask > 0) if person_mask is not None else False,
                "pool_detected": np.any(pool_mask > 0) if pool_mask is not None else False,
                "num_segments": segment_result["num_segments"],
                "saved_images": saved_files,
                "segment_info": segment_result.get("segments", [])
            }
            
            json_filename = f"frame_{frame_number}_sam_analysis.json"
            json_path = os.path.join(self.sam_dir, json_filename)
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(json_info, f, indent=2, ensure_ascii=False)
            
            saved_files.append(json_filename)
            
        except Exception as e:
            logger.error(f"ä¿å­˜å¤„ç†ç»“æœå¤±è´¥: {str(e)}")
        
        return saved_files
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """
        è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            # ç»Ÿè®¡å„ç±»æ–‡ä»¶æ•°é‡
            if not os.path.exists(self.sam_dir):
                return {"error": "SAMç›®å½•ä¸å­˜åœ¨"}
            
            files = os.listdir(self.sam_dir)
            
            person_only_images = [f for f in files if 'person_only' in f and f.endswith('.jpg')]
            pool_only_images = [f for f in files if 'pool_only' in f and f.endswith('.jpg')]
            combined_images = [f for f in files if 'combined' in f and f.endswith('.jpg')]
            highlighted_images = [f for f in files if 'highlighted_person' in f and f.endswith('.jpg')]
            mask_viz_images = [f for f in files if 'mask_visualization' in f and f.endswith('.jpg')]
            analysis_jsons = [f for f in files if 'sam_analysis' in f and f.endswith('.json')]
            
            return {
                "person_only_images": len(person_only_images),
                "pool_only_images": len(pool_only_images),
                "combined_images": len(combined_images),
                "highlighted_images": len(highlighted_images),
                "mask_visualizations": len(mask_viz_images),
                "analysis_files": len(analysis_jsons),
                "total_files": len(files),
                "output_directory": self.sam_dir,
                "latest_files": {
                    "recent_combined": combined_images[-3:] if combined_images else [],
                    "recent_analysis": analysis_jsons[-3:] if analysis_jsons else []
                }
            }
            
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {"error": str(e)}

# å…¨å±€å¤„ç†å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_sam_processor_instance = None

def get_sam_processor(output_dir: str = "output") -> SAMProcessor:
    """
    è·å–SAMå¤„ç†å™¨å®ä¾‹ï¼ˆå•ä¾‹ï¼‰
    
    Args:
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        SAMå¤„ç†å™¨å®ä¾‹
    """
    global _sam_processor_instance
    
    if _sam_processor_instance is None:
        _sam_processor_instance = SAMProcessor(output_dir)
    
    return _sam_processor_instance 